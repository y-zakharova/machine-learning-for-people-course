{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Секція 1. Логістична регресія з нуля.**\n",
        "\n",
        "Будемо крок за кроком будувати модель лог регресії з нуля для передбачення, чи буде врожай більше за 80 яблук (задача подібна до лекційної, але на класифікацію).\n",
        "\n",
        "Давайте нагадаємо основні формули для логістичної регресії.\n",
        "\n",
        "### Функція гіпотези - обчислення передбачення у логістичній регресії:\n",
        "\n",
        "$$\n",
        "\\hat{y} = \\sigma(x W^T + b) = \\frac{1}{1 + e^{-(x W^T + b)}}\n",
        "$$\n",
        "\n",
        "Де:\n",
        "- $ \\hat{y} $ — це ймовірність \"позитивного\" класу.\n",
        "- $ x $ — це вектор (або матриця для набору прикладів) вхідних даних.\n",
        "- $ W $ — це вектор (або матриця) вагових коефіцієнтів моделі.\n",
        "- $ b $ — це зміщення (bias).\n",
        "- $ \\sigma(z) $ — це сигмоїдна функція активації.\n",
        "\n",
        "### Як обчислюється сигмоїдна функція:\n",
        "\n",
        "Сигмоїдна функція $ \\sigma(z) $ має вигляд:\n",
        "\n",
        "$$\n",
        "\\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
        "$$\n",
        "\n",
        "Ця функція перетворює будь-яке дійсне значення $ z $ в інтервал від 0 до 1, що дозволяє інтерпретувати вихід як ймовірність для логістичної регресії.\n",
        "\n",
        "### Формула функції втрат для логістичної регресії (бінарна крос-ентропія):\n",
        "\n",
        "Функція втрат крос-ентропії оцінює, наскільки добре модель передбачає класи, порівнюючи передбачені ймовірності $ \\hat{y} $ із справжніми мітками $ y $. Формула наступна:\n",
        "\n",
        "$$\n",
        "L(y, \\hat{y}) = - \\left[ y \\cdot \\log(\\hat{y}) + (1 - y) \\cdot \\log(1 - \\hat{y}) \\right]\n",
        "$$\n",
        "\n",
        "Де:\n",
        "- $ y $ — це справжнє значення (мітка класу, 0 або 1).\n",
        "- $ \\hat{y} $ — це передбачене значення (ймовірність).\n",
        "\n"
      ],
      "metadata": {
        "id": "lbLHTNfSclli"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.\n",
        "Тут вже наведений код для ініціювання набору даних в форматі numpy. Перетворіть `inputs`, `targets` на `torch` тензори. Виведіть результат на екран."
      ],
      "metadata": {
        "id": "GtOYB-RHfc_r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "3BNXSR-VdYKQ"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "QLKZ77x4v_-v"
      },
      "outputs": [],
      "source": [
        "# Вхідні дані (temp, rainfall, humidity)\n",
        "inputs = np.array([[73, 67, 43],\n",
        "                   [91, 88, 64],\n",
        "                   [87, 134, 58],\n",
        "                   [102, 43, 37],\n",
        "                   [69, 96, 70]], dtype='float32')\n",
        "\n",
        "# Таргети (apples > 80)\n",
        "targets = np.array([[0],\n",
        "                    [1],\n",
        "                    [1],\n",
        "                    [0],\n",
        "                    [1]], dtype='float32')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = torch.from_numpy(inputs)\n",
        "inputs"
      ],
      "metadata": {
        "id": "KjoeaDrk6fO7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34e50543-a1a0-4c1c-ac3a-67ddacb2e768"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 73.,  67.,  43.],\n",
              "        [ 91.,  88.,  64.],\n",
              "        [ 87., 134.,  58.],\n",
              "        [102.,  43.,  37.],\n",
              "        [ 69.,  96.,  70.]])"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "targets = torch.from_numpy(targets)\n",
        "targets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VjGuwBfOrEe1",
        "outputId": "df317783-00a1-49c6-cfd0-ef71f5b8ed19"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [0.],\n",
              "        [1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Ініціюйте ваги `w`, `b` для моделі логістичної регресії потрібної форми зважаючи на розмірності даних випадковими значеннями з нормального розподілу. Лишаю тут код для фіксації `random_seed`."
      ],
      "metadata": {
        "id": "iKzbJKfOgGV8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.random.manual_seed(1)"
      ],
      "metadata": {
        "id": "aXhKw6Tdj1-d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "963a801b-d73c-4b55-ddda-b02ecef76741"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7bd0f37e0d30>"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "w = torch.randn(1, 3, requires_grad=True)\n",
        "b = torch.randn(1, requires_grad=True)\n",
        "\n",
        "print(w, b)"
      ],
      "metadata": {
        "id": "eApcB7eb6h9o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02da1076-27aa-49b9-ce8f-29f6f6a1ee44"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-0.4519, -0.1661, -1.5228]], requires_grad=True) tensor([0.3817], requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Напишіть функцію `model`, яка буде обчислювати функцію гіпотези в логістичній регресії і дозволяти робити передбачення на основі введеного рядка даних і коефіцієнтів в змінних `w`, `b`.\n",
        "\n",
        "  **Важливий момент**, що функція `model` робить обчислення на `torch.tensors`, тож для математичних обчислень використовуємо фукнціонал `torch`, наприклад:\n",
        "  - обчсилення $e^x$: `torch.exp(x)`\n",
        "  - обчсилення $log(x)$: `torch.log(x)`\n",
        "  - обчислення середнього значення вектору `x`: `torch.mean(x)`\n",
        "\n",
        "  Використайте функцію `model` для обчислення передбачень з поточними значеннями `w`, `b`.Виведіть результат обчислень на екран.\n",
        "\n",
        "  Проаналізуйте передбачення. Чи не викликають вони у вас підозр? І якщо викликають, то чим це може бути зумовлено?"
      ],
      "metadata": {
        "id": "nYGxNGTaf5s6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def model(x, w, b):\n",
        "  z = x @ w.t() + b\n",
        "  return 1 / (1 + torch.exp(-z))"
      ],
      "metadata": {
        "id": "pSz2j4Fh6jBv"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model(inputs, w, b)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KcYbMxhlsYsZ",
        "outputId": "fc8dc606-2071-49df-d667-423981882607"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.],\n",
              "        [0.],\n",
              "        [0.],\n",
              "        [0.],\n",
              "        [0.]], grad_fn=<MulBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "targets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eWkAZTK0tN4B",
        "outputId": "39575db2-f870-48a5-dae9-d48579c26669"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [0.],\n",
              "        [1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Результати викликають підозри, оскільки всюди передбачається одне значення, а також те, що виходять цілі числа (е в дуже маленькій степені повертає число, близьке до 0 і не вистачає памʼяті, щоб записати числа після коми)."
      ],
      "metadata": {
        "id": "wbu59R79tKtw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Напишіть функцію `binary_cross_entropy`, яка приймає на вхід передбачення моделі `predicted_probs` та справжні мітки в даних `true_labels` і обчислює значення втрат (loss)  за формулою бінарної крос-ентропії для кожного екземпляра та вертає середні втрати по всьому набору даних.\n",
        "  Використайте функцію `binary_cross_entropy` для обчислення втрат для поточних передбачень моделі."
      ],
      "metadata": {
        "id": "O2AGM0Mb2yHa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def binary_cross_entropy(predicted_probs, true_labels):\n",
        "  eps = 1e-7 # to avoid zero division\n",
        "  loss = -1 * (true_labels * torch.log(eps + predicted_probs) + (1 - true_labels) * torch.log(1 - predicted_probs + eps))\n",
        "  return torch.mean(loss)"
      ],
      "metadata": {
        "id": "1bWlovvx6kZS"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss = binary_cross_entropy(model(inputs, w, b), targets)\n",
        "loss"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9KmTjmptuJsu",
        "outputId": "1e02ac24-88bd-4d6c-e3e3-46ca2e4678ac"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(9.6709, grad_fn=<MeanBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Зробіть зворотнє поширення помилки і виведіть градієнти за параметрами `w`, `b`. Проаналізуйте їх значення. Як гадаєте, чому вони саме такі?"
      ],
      "metadata": {
        "id": "ZFKpQxdHi1__"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss.backward()"
      ],
      "metadata": {
        "id": "YAbXUNSJ6mCl"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w.grad"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JSY9RMcJxLB3",
        "outputId": "33a71b84-bb7c-44dd-c269-2fb06daa2625"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[nan, nan, nan]])"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "b.grad"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TnXyD5ksxNuk",
        "outputId": "041e77ae-4556-4c71-d069-d1501725e37e"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([nan])"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Я думаю, що проблема у рандомному сеті ваг."
      ],
      "metadata": {
        "id": "lo_UYaWSxPs3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Що сталось?**\n",
        "\n",
        "В цій задачі, коли ми ініціювали значення випадковими значеннями з нормального розподілу, насправді ці значення не були дуже гарними стартовими значеннями і привели до того, що градієнти стали дуже малими або навіть рівними нулю (це призводить до того, що градієнти \"зникають\"), і відповідно при оновленні ваг у нас не буде нічого змінюватись. Це називається `gradient vanishing`. Це відбувається через **насичення сигмоїдної функції активації.**\n",
        "\n",
        "У нашій задачі ми використовуємо сигмоїдну функцію активації, яка має такий вигляд:\n",
        "\n",
        "   $$\n",
        "   \\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
        "   $$\n",
        "\n",
        "\n",
        "Коли значення $z$ дуже велике або дуже мале, сигмоїдна функція починає \"насичуватись\". Це означає, що для великих позитивних $z$ сигмоїда наближається до 1, а для великих негативних — до 0. В цих діапазонах градієнти починають стрімко зменшуватись і наближаються до нуля (бо градієнт - це похідна, похідна на проміжку функції, де вона паралельна осі ОХ, дорівнює 0), що робить оновлення ваг неможливим.\n",
        "\n",
        "![](https://editor.analyticsvidhya.com/uploads/27889vaegp.png)\n",
        "\n",
        "У логістичній регресії $ z = x \\cdot w + b $. Якщо ваги $w, b$ - великі, значення $z$ також буде великим, і сигмоїда перейде в насичену область, де градієнти дуже малі.\n",
        "\n",
        "Саме це сталося в нашій задачі, де великі випадкові значення ваг викликали насичення сигмоїдної функції. Це в свою чергу призводить до того, що під час зворотного поширення помилки (backpropagation) модель оновлює ваги дуже повільно або зовсім не оновлює. Це називається проблемою **зникнення градієнтів** (gradient vanishing problem).\n",
        "\n",
        "**Що ж робити?**\n",
        "Ініціювати ваги маленькими значеннями навколо нуля. Наприклад ми можемо просто в існуючій ініціалізації ваги розділити на 1000. Можна також використати інший спосіб ініціалізації вагів - інформація про це [тут](https://www.geeksforgeeks.org/initialize-weights-in-pytorch/).\n",
        "\n",
        "Як це робити - показую нижче. **Виконайте код та знову обчисліть передбачення, лосс і виведіть градієнти.**\n",
        "\n",
        "А я пишу пояснення, чому просто не зробити\n",
        "\n",
        "```\n",
        "w = torch.randn(1, 3, requires_grad=True)/1000\n",
        "b = torch.randn(1, requires_grad=True)/1000\n",
        "```\n",
        "\n",
        "Нам потрібно, аби тензори вагів були листовими (leaf tensors).\n",
        "\n",
        "1. **Що таке листовий тензор**\n",
        "Листовий тензор — це тензор, який був створений користувачем безпосередньо і з якого починається обчислювальний граф. Якщо такий тензор має `requires_grad=True`, PyTorch буде відслідковувати всі операції, виконані над ним, щоб правильно обчислювати градієнти під час навчання.\n",
        "\n",
        "2. **Чому ми використовуємо `w.data` замість звичайних операцій**\n",
        "Якщо ми просто виконали б операції, такі як `(w - 0.5) / 100`, ми б отримали **новий тензор**, який вже не був би листовим тензором, оскільки ці операції створюють **новий** тензор, а не модифікують існуючий.\n",
        "\n",
        "  Проте, щоб залишити наші тензори ваги `w` та зміщення `b` листовими і продовжити можливість відстеження градієнтів під час тренування, ми використовуємо атрибут `.data`. Цей атрибут дозволяє **виконувати операції in-place (прямо на існуючому тензорі)** без зміни самого об'єкта тензора. Отже, тензор залишається листовим, і PyTorch може коректно обчислювати його градієнти.\n",
        "\n",
        "3. **Чому важливо залишити тензор листовим**\n",
        "Якщо тензор більше не є листовим (наприклад, через проведення операцій, що створюють нові тензори), ви не зможете отримати градієнти за допомогою `w.grad` чи `b.grad` після виклику `loss.backward()`. Це може призвести до втрати можливості оновлення параметрів під час тренування моделі. В нашому випадку ми хочемо, щоб тензори `w` та `b` накопичували градієнти, тому вони повинні залишатись листовими.\n",
        "\n",
        "**Висновок:**\n",
        "Ми використовуємо `.data`, щоб виконати операції зміни значень на ваги і зміщення **in-place**, залишаючи їх листовими тензорами, які можуть накопичувати градієнти під час навчання. Це дозволяє коректно працювати механізму зворотного поширення помилки (backpropagation) і оновлювати ваги моделі."
      ],
      "metadata": {
        "id": "nDN1t1RujQsK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Виконайте код та знову обчисліть передбачення, лосс і знайдіть градієнти та виведіть всі ці тензори на екран."
      ],
      "metadata": {
        "id": "rOPSQyttpVjO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.random.manual_seed(1)\n",
        "w = torch.randn(1, 3, requires_grad=True)  # Листовий тензор\n",
        "b = torch.randn(1, requires_grad=True)     # Листовий тензор\n",
        "\n",
        "# in-place операції\n",
        "w.data = w.data / 1000\n",
        "b.data = b.data / 1000"
      ],
      "metadata": {
        "id": "-EBOJ3tsnRaD"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model(inputs, w, b)"
      ],
      "metadata": {
        "id": "-JwXiSpX6orh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5b9dffd-80ef-450c-915c-ac7ff159e8d1"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.5174],\n",
              "        [0.5220],\n",
              "        [0.5244],\n",
              "        [0.5204],\n",
              "        [0.5190]], grad_fn=<MulBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss = binary_cross_entropy(model(inputs, w, b), targets)\n",
        "loss"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rhvh5QqgycGI",
        "outputId": "5b47038f-291d-4ed6-b289-ef1a19e73f02"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.6829, grad_fn=<MeanBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss.backward()"
      ],
      "metadata": {
        "id": "krdeBqj8yfyU"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w.grad"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n5hOTMIrygNB",
        "outputId": "4fdd5e42-0225-41b6-b308-044381232001"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ -5.4417, -18.9853, -10.0682]])"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "b.grad"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2INrpZRayizl",
        "outputId": "54683f8a-8517-414c-c9cc-07d401ffbb2e"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([-0.0794])"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Напишіть алгоритм градієнтного спуску, який буде навчати модель з використанням написаних раніше функцій і виконуючи оновлення ваг. Алгоритм має включати наступні кроки:\n",
        "\n",
        "    1. Генерація прогнозів\n",
        "    2. Обчислення втрат\n",
        "    3. Обчислення градієнтів (gradients) loss-фукнції відносно ваг і зсувів\n",
        "    4. Налаштування ваг шляхом віднімання невеликої величини, пропорційної градієнту (`learning_rate` домножений на градієнт)\n",
        "    5. Скидання градієнтів на нуль\n",
        "\n",
        "\n",
        "\n",
        "Виконайте градієнтний спуск протягом 1000 епох, обчисліть фінальні передбачення і проаналізуйте, чи вони точні?"
      ],
      "metadata": {
        "id": "RCdi44IT334o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 1e-5\n",
        "\n",
        "for i in range(1000):\n",
        "    preds = model(inputs, w, b)\n",
        "    loss = binary_cross_entropy(preds, targets)\n",
        "    loss.backward()\n",
        "    with torch.no_grad():\n",
        "        w -= w.grad * learning_rate\n",
        "        b -= b.grad * learning_rate\n",
        "        w.grad.zero_()\n",
        "        b.grad.zero_()"
      ],
      "metadata": {
        "id": "mObHPyE06qsO"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preds = model(inputs, w, b)\n",
        "loss = binary_cross_entropy(preds, targets)\n",
        "print(loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DE4CqIjPzJKd",
        "outputId": "cafbd7cd-a2c3-4311-e2c6-92b904f6fae9"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.3357, grad_fn=<MeanBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "preds"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kJiK9U4czMeO",
        "outputId": "f83a3fec-230f-441b-b406-b1c4b340d5f0"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.5777],\n",
              "        [0.6685],\n",
              "        [0.9113],\n",
              "        [0.1616],\n",
              "        [0.8653]], grad_fn=<MulBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "targets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ppa2rLDhzN-1",
        "outputId": "4fcc5f13-d2ac-4db0-f21e-bb0960ca77c2"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [0.],\n",
              "        [1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Передбачення непогані, але треба попрацювати над тим, який трешхолд для визначення 1 (наприклад, 0.65)"
      ],
      "metadata": {
        "id": "JTBlEAjVzTLy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Секція 2. Створення лог регресії з використанням функціоналу `torch.nn`.**\n",
        "\n",
        "Давайте повторно реалізуємо ту ж модель, використовуючи деякі вбудовані функції та класи з PyTorch.\n",
        "\n",
        "Даних у нас буде побільше - тож, визначаємо нові масиви."
      ],
      "metadata": {
        "id": "fuRhlyF9qAia"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Вхідні дані (temp, rainfall, humidity)\n",
        "inputs = np.array([[73, 67, 43],\n",
        "                   [91, 88, 64],\n",
        "                   [87, 134, 58],\n",
        "                   [102, 43, 37],\n",
        "                   [69, 96, 70],\n",
        "                   [73, 67, 43],\n",
        "                   [91, 88, 64],\n",
        "                   [87, 134, 58],\n",
        "                   [102, 43, 37],\n",
        "                   [69, 96, 70],\n",
        "                   [73, 67, 43],\n",
        "                   [91, 88, 64],\n",
        "                   [87, 134, 58],\n",
        "                   [102, 43, 37],\n",
        "                   [69, 96, 70]], dtype='float32')\n",
        "\n",
        "# Таргети (apples > 80)\n",
        "targets = np.array([[0],\n",
        "                    [1],\n",
        "                    [1],\n",
        "                    [0],\n",
        "                    [1],\n",
        "                    [0],\n",
        "                    [1],\n",
        "                    [1],\n",
        "                    [0],\n",
        "                    [1],\n",
        "                    [0],\n",
        "                    [1],\n",
        "                    [1],\n",
        "                    [0],\n",
        "                    [1]], dtype='float32')"
      ],
      "metadata": {
        "id": "IX8Bhm74rV4M"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Завантажте вхідні дані та мітки в PyTorch тензори та з них створіть датасет, який поєднує вхідні дані з мітками, використовуючи клас `TensorDataset`. Виведіть перші 3 елементи в датасеті.\n",
        "\n"
      ],
      "metadata": {
        "id": "7X2dV30KtAPu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader"
      ],
      "metadata": {
        "id": "22pedywBzwH8"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = torch.from_numpy(inputs)\n",
        "targets = torch.from_numpy(targets)\n",
        "\n",
        "train_ds = TensorDataset(inputs, targets)\n",
        "train_ds[:3]"
      ],
      "metadata": {
        "id": "chrvMfBs6vjo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab209c2c-3e45-4811-93f7-55b701eb862f"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[ 73.,  67.,  43.],\n",
              "         [ 91.,  88.,  64.],\n",
              "         [ 87., 134.,  58.]]),\n",
              " tensor([[0.],\n",
              "         [1.],\n",
              "         [1.]]))"
            ]
          },
          "metadata": {},
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Визначте data loader з класом **DataLoader** для підготовленого датасету `train_ds`, встановіть розмір батчу на 5 та увімкніть перемішування даних для ефективного навчання моделі. Виведіть перший елемент в дата лоадері."
      ],
      "metadata": {
        "id": "4nMFaa8suOd3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 5\n",
        "train_dl = DataLoader(train_ds, batch_size, shuffle=True)\n",
        "\n",
        "next(iter(train_dl))"
      ],
      "metadata": {
        "id": "ZCsRo5Mx6wEI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ccb8dcb-b96a-4212-e358-a3efe66b9a0e"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[tensor([[ 73.,  67.,  43.],\n",
              "         [102.,  43.,  37.],\n",
              "         [ 91.,  88.,  64.],\n",
              "         [ 73.,  67.,  43.],\n",
              "         [ 87., 134.,  58.]]),\n",
              " tensor([[0.],\n",
              "         [0.],\n",
              "         [1.],\n",
              "         [0.],\n",
              "         [1.]])]"
            ]
          },
          "metadata": {},
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. Створіть клас `LogReg` для логістичної регресії, наслідуючи модуль `torch.nn.Module` за прикладом в лекції (в частині про FeedForward мережі).\n",
        "\n",
        "  У нас модель складається з лінійної комбінації вхідних значень і застосування фукнції сигмоїда. Тож, нейромережа буде складатись з лінійного шару `nn.Linear` і використання активації `nn.Sigmid`. У створеному класі мають бути реалізовані методи `__init__` з ініціалізацією шарів і метод `forward` для виконання прямого проходу моделі через лінійний шар і функцію активації.\n",
        "\n",
        "  Створіть екземпляр класу `LogReg` в змінній `model`."
      ],
      "metadata": {
        "id": "ymcQOo_hum6I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "3DyMDdkj0CLZ"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LogReg(nn.Module):\n",
        "  def __init__(self):\n",
        "     super().__init__()\n",
        "     self.linear = nn.Linear(3, 1)\n",
        "     self.act = nn.Sigmoid()\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.linear(x)\n",
        "    x = self.act(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "EyAwhTBW6xxz"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = LogReg()"
      ],
      "metadata": {
        "id": "UDDgX2ky0pmf"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. Задайте оптимізатор `Stockastic Gradient Descent` в змінній `opt` для навчання моделі логістичної регресії. А також визначіть в змінній `loss` функцію втрат `binary_cross_entropy` з модуля `torch.nn.functional` для обчислення втрат моделі. Обчисліть втрати для поточних передбачень і міток, а потім виведіть їх. Зробіть висновок, чи моделі вдалось навчитись?"
      ],
      "metadata": {
        "id": "RflV7xeVyoJy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "opt = torch.optim.SGD(model.parameters(), 1e-5)\n",
        "loss = F.binary_cross_entropy"
      ],
      "metadata": {
        "id": "3QCATPU_6yfa"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss(model(inputs), targets)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sp46xBrS0_f5",
        "outputId": "387238d8-50bb-4754-fc7b-b442c51f3f8d"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(4.2120, grad_fn=<BinaryCrossEntropyBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Виглядає, що лосс високий, треба ще довчитись."
      ],
      "metadata": {
        "id": "_lmY5ZZQ1beg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "11. Візьміть з лекції функцію для тренування моделі з відстеженням значень втрат і навчіть щойно визначену модель на 1000 епохах. Виведіть після цього графік зміни loss, фінальні передбачення і значення таргетів."
      ],
      "metadata": {
        "id": "ch-WrYnKzMzq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fit_return_loss(num_epochs, model, loss_fn, opt, train_dl):\n",
        "    losses = []\n",
        "    for epoch in range(num_epochs):\n",
        "        total_loss = 0\n",
        "\n",
        "        for xb, yb in train_dl:\n",
        "            pred = model(xb)\n",
        "\n",
        "            loss = loss_fn(pred, yb)\n",
        "\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "            opt.zero_grad()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        avg_loss = total_loss / len(train_dl)\n",
        "        losses.append(avg_loss)\n",
        "\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "          print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}')\n",
        "    return losses"
      ],
      "metadata": {
        "id": "cEHQH9qE626k"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss = fit_return_loss(1000, model, loss, opt, train_dl)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0lvlOzVF1nv7",
        "outputId": "1faf7f27-fbf5-47b0-be4c-e6d4a065d31f"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [10/1000], Loss: 4.0676\n",
            "Epoch [20/1000], Loss: 3.9264\n",
            "Epoch [30/1000], Loss: 3.8004\n",
            "Epoch [40/1000], Loss: 3.6843\n",
            "Epoch [50/1000], Loss: 3.5940\n",
            "Epoch [60/1000], Loss: 3.5054\n",
            "Epoch [70/1000], Loss: 3.4346\n",
            "Epoch [80/1000], Loss: 3.3712\n",
            "Epoch [90/1000], Loss: 3.3097\n",
            "Epoch [100/1000], Loss: 3.2474\n",
            "Epoch [110/1000], Loss: 3.1864\n",
            "Epoch [120/1000], Loss: 3.1215\n",
            "Epoch [130/1000], Loss: 3.0614\n",
            "Epoch [140/1000], Loss: 3.0165\n",
            "Epoch [150/1000], Loss: 2.9422\n",
            "Epoch [160/1000], Loss: 2.8888\n",
            "Epoch [170/1000], Loss: 2.8213\n",
            "Epoch [180/1000], Loss: 2.7633\n",
            "Epoch [190/1000], Loss: 2.7048\n",
            "Epoch [200/1000], Loss: 2.6668\n",
            "Epoch [210/1000], Loss: 2.6023\n",
            "Epoch [220/1000], Loss: 2.5350\n",
            "Epoch [230/1000], Loss: 2.4787\n",
            "Epoch [240/1000], Loss: 2.4151\n",
            "Epoch [250/1000], Loss: 2.3578\n",
            "Epoch [260/1000], Loss: 2.2964\n",
            "Epoch [270/1000], Loss: 2.2421\n",
            "Epoch [280/1000], Loss: 2.1875\n",
            "Epoch [290/1000], Loss: 2.1311\n",
            "Epoch [300/1000], Loss: 2.0704\n",
            "Epoch [310/1000], Loss: 2.0186\n",
            "Epoch [320/1000], Loss: 1.9636\n",
            "Epoch [330/1000], Loss: 1.9063\n",
            "Epoch [340/1000], Loss: 1.8502\n",
            "Epoch [350/1000], Loss: 1.7940\n",
            "Epoch [360/1000], Loss: 1.7534\n",
            "Epoch [370/1000], Loss: 1.6993\n",
            "Epoch [380/1000], Loss: 1.6375\n",
            "Epoch [390/1000], Loss: 1.5859\n",
            "Epoch [400/1000], Loss: 1.5370\n",
            "Epoch [410/1000], Loss: 1.4831\n",
            "Epoch [420/1000], Loss: 1.4403\n",
            "Epoch [430/1000], Loss: 1.3862\n",
            "Epoch [440/1000], Loss: 1.3444\n",
            "Epoch [450/1000], Loss: 1.3041\n",
            "Epoch [460/1000], Loss: 1.2525\n",
            "Epoch [470/1000], Loss: 1.2125\n",
            "Epoch [480/1000], Loss: 1.1693\n",
            "Epoch [490/1000], Loss: 1.1303\n",
            "Epoch [500/1000], Loss: 1.0947\n",
            "Epoch [510/1000], Loss: 1.0669\n",
            "Epoch [520/1000], Loss: 1.0253\n",
            "Epoch [530/1000], Loss: 0.9969\n",
            "Epoch [540/1000], Loss: 0.9590\n",
            "Epoch [550/1000], Loss: 0.9346\n",
            "Epoch [560/1000], Loss: 0.9068\n",
            "Epoch [570/1000], Loss: 0.8855\n",
            "Epoch [580/1000], Loss: 0.8584\n",
            "Epoch [590/1000], Loss: 0.8313\n",
            "Epoch [600/1000], Loss: 0.8238\n",
            "Epoch [610/1000], Loss: 0.7910\n",
            "Epoch [620/1000], Loss: 0.7751\n",
            "Epoch [630/1000], Loss: 0.7581\n",
            "Epoch [640/1000], Loss: 0.7378\n",
            "Epoch [650/1000], Loss: 0.7224\n",
            "Epoch [660/1000], Loss: 0.7080\n",
            "Epoch [670/1000], Loss: 0.6972\n",
            "Epoch [680/1000], Loss: 0.6853\n",
            "Epoch [690/1000], Loss: 0.6732\n",
            "Epoch [700/1000], Loss: 0.6626\n",
            "Epoch [710/1000], Loss: 0.6505\n",
            "Epoch [720/1000], Loss: 0.6404\n",
            "Epoch [730/1000], Loss: 0.6398\n",
            "Epoch [740/1000], Loss: 0.6234\n",
            "Epoch [750/1000], Loss: 0.6259\n",
            "Epoch [760/1000], Loss: 0.6073\n",
            "Epoch [770/1000], Loss: 0.6031\n",
            "Epoch [780/1000], Loss: 0.5936\n",
            "Epoch [790/1000], Loss: 0.5866\n",
            "Epoch [800/1000], Loss: 0.5807\n",
            "Epoch [810/1000], Loss: 0.5742\n",
            "Epoch [820/1000], Loss: 0.5691\n",
            "Epoch [830/1000], Loss: 0.5660\n",
            "Epoch [840/1000], Loss: 0.5621\n",
            "Epoch [850/1000], Loss: 0.5540\n",
            "Epoch [860/1000], Loss: 0.5511\n",
            "Epoch [870/1000], Loss: 0.5480\n",
            "Epoch [880/1000], Loss: 0.5449\n",
            "Epoch [890/1000], Loss: 0.5388\n",
            "Epoch [900/1000], Loss: 0.5331\n",
            "Epoch [910/1000], Loss: 0.5285\n",
            "Epoch [920/1000], Loss: 0.5273\n",
            "Epoch [930/1000], Loss: 0.5222\n",
            "Epoch [940/1000], Loss: 0.5191\n",
            "Epoch [950/1000], Loss: 0.5150\n",
            "Epoch [960/1000], Loss: 0.5130\n",
            "Epoch [970/1000], Loss: 0.5104\n",
            "Epoch [980/1000], Loss: 0.5062\n",
            "Epoch [990/1000], Loss: 0.5048\n",
            "Epoch [1000/1000], Loss: 0.5009\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(loss)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "cQENFPlI1qzM",
        "outputId": "9f7b90f4-8952-428d-cd6d-e7605f5e7114"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model(inputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D1s_KRET11-m",
        "outputId": "2c6177f8-79c0-4ed2-e316-7cc9222e31a5"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.7834],\n",
              "        [0.5770],\n",
              "        [0.9998],\n",
              "        [0.1064],\n",
              "        [0.7347],\n",
              "        [0.7834],\n",
              "        [0.5770],\n",
              "        [0.9998],\n",
              "        [0.1064],\n",
              "        [0.7347],\n",
              "        [0.7834],\n",
              "        [0.5770],\n",
              "        [0.9998],\n",
              "        [0.1064],\n",
              "        [0.7347]], grad_fn=<SigmoidBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "targets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3883s1CE2M3p",
        "outputId": "5eff1717-17f0-43b8-f42d-d8e71ca2eec5"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [0.],\n",
              "        [1.],\n",
              "        [0.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [0.],\n",
              "        [1.],\n",
              "        [0.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [0.],\n",
              "        [1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Загалом, виглядає доволі непогано, але є питання до деяких значень (думаю, це ще можна тюнити)."
      ],
      "metadata": {
        "id": "aejAClj72RPn"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9NukTtdt2h4R"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
